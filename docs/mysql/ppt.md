> 环境：
>
> mysql:5.7
>
> InnoDB（Repeatable Read）

# 索引

## 数据结构/B+Tree

### 科普

> - **概念：** B+Tree中的B是balance，指的是平衡，它叫平衡多路查找树。它是为`磁盘`等外设设备设计的一种`平衡查找树`，磁盘的I/O速度一直是数据库的瓶颈所在，针对一次增删改查，越少的磁盘I/O就越有利于性能。
> - **磁盘的读取机制：** 简单来说磁盘每次读取的大小为`页(4KB)的整数倍`，哪怕只需要读取1B的内容也是读取了4KB的页，内存与磁盘以页为单位交换数据。 
> - **InnoDB中的页：** InnoDB中默认每个页的大小为16kb，可以通过设置参数innodb_page_size来将页的大小设置为4k、8k、16k，即为磁盘页大小的整数倍。

> 下图中是一颗三阶B+Tree
>
> ![img](/Users/wang/dev/environment/临时截图/QQ20191205-150910.png)
>
> - 图中，页33、页30等表示的是`InnoDB中的“页”`，在磁盘中的表现形式也是磁盘页（例如InnoDB中设置页大小为8kb，那么一个DB的页在磁盘中就是两个连续地址的磁盘页）
> - 在B+树的非叶节点中不存储数据，如磁页30、33、32中，只顺序存放了`页地址`和`主键`的键值。所有的数据都存储在叶子节点中。并且最底层的叶子节点是一个`双向链表`。
> - 得益于非叶节点只存放页地址和主键的值，每个非叶节点能够存放大量的主键，那么整棵树相当于变胖了，降低了深度，减少IO次数从而提高性能。
>
> **做一个简单的推算：**
>
> > InnoDB存储引擎中页的大小为16KB，一般表的主键类型为INT（占用4B）或BIGINT（占用8B），磁盘地址也一般为4或8B，都按照8B计算，也就是说一个页（B+Tree中的一个非叶节点）中大概存储16KB/(8B+8B)=1000个键值（因为是估值，为方便计算，这里的进制转换取值为10^3）。也就是说一个深度为3的B+Tree索引至少可以维护10^3 * 10^3 * 10^3 = 10亿 条记录。因此主键通常不推荐使用长度特别长的列。
>
> **模拟查找主键为10的记录：**
>
> > 1. 读取根节点(页33)，载入内存。 **【第一次磁盘IO】**
> > 2.  1 < 10 < 320，获得指向页30的地址。
> > 3. 读取页30，载入内存。 **【第二次磁盘IO】**
> > 4. 比较主键的大小，5 < 10 < 12，获得页28的地址。
> > 5. 读取页28，载入内存。 **【第三次磁盘IO】**
> > 6. 查找到主键为10 的记录
>
> **非聚集索引树**
>
> > - **聚簇索引：** 索引的逻辑顺序即为记录的物理顺序，即主键。
> > - **非聚簇索引：** 反之索引列的逻辑顺序与记录的物理顺序不同的就是非聚集索引。如普通索引、唯一索引、全文索引。
> >
> > 非聚集索引树的结构也是B+树，和上文中的聚集索引树完全相同，唯一不同的是，`聚集索引树的叶子节点存放的是每条记录的数据，非聚集索引树存放的是主键的值。`所以每次根据非主键的索引进行查询时，多半需要回表操作。
> >
> > **模拟查找普通索引为2的记录：**
> >
> > ![image-20191205153457874](/Users/wang/dev/environment/临时截图/image-20191205153457874.png)
> >
> > - 读取页44，对比2 < 4 < 9，获得页42地址     **【第一次IO】**
> > - 读取页42，获得页34地址                              **【第二次IO】**
> > - 读取页34，找到索引为2的记录对应主键为20**【第三次IO】**
> > - 以上文中的聚集索引树为例，查找主键为20的记录  **【共六次IO】**
> >
> > 可以发现使用非聚集索引的IO次数比聚集索引要多，所以会比它慢。

## 为什么索引能提升查询的速度？

> 这里有一个前提，是SQL语句能够正确的触发索引。索引之所以能提升检索速度是因为每个索引都有对应的一颗B+树，在B+树上该索引是有序排列的，因此可以使用搜索算法快速的找到指定列。

### 复合索引

> - **概念：**复合索引又叫联合索引，由两个或两个以上的列组成的索引称为联合索引。
>
> **例子：**
>
> ```sql
> CREATE TABLE `t1` (
>   `id` int(11) NOT NULL AUTO_INCREMENT,
>   `a` int(255) ,
>   `b` int(255) ,
>   `c` int(255) ,
>   `d` int(255) ,
>   PRIMARY KEY (`id`),
>   KEY `index` (`a`)
> ) 
> ```
>
> 表内假设有500W条数据，a = 1的有100W条，此时执行
>
> ```sql
> select * from t1 where a = 1 and b = 2 and c = 3;
> ```
>
> 此时查询的步骤为：
>
> > 1. 从索引a的B+树中获得a = 1的所有记录的主键，共100W条
> > 2. 根据这100W条主键回表查询出对应的记录，再筛选出b = 2，c = 3的记录。看起来效率并不乐观
> >
> > ![](/Users/wang/dev/environment/临时截图/QQ20191209-154841.png)
>
> 若针对表中a、b、c三个列创建了联合主键后查询的步骤为：
>
> > 1. 从符合索引(a,b,c)的B+树中获得条件的所有记录的主键
> > 2. 回表查询数据
> >
> > ![](/Users/wang/dev/environment/临时截图/QQ20191209-155020.png)
>
> **复合索引适合在由多个列来确定一行或者多行数据时使用**
>
> > 例如确定一个游戏的角色，需要从游戏账号-->游戏大区-->角色名，多个条件确定。
>
> **最左匹配原则：**
>
> > 在日常开发使用多个条件进行查询时，出现的索引失效多半和最左匹配原则相关。
>
> **例子：**
>
> > ```sql
> > CREATE TABLE `t2` (
> >   `id` int(11) NOT NULL AUTO_INCREMENT,
> >   `a` int(255) ,
> >   `b` int(255) ,
> >   `c` int(255) ,
> >   `d` int(255) ,
> >   PRIMARY KEY (`id`),
> >   KEY `index` (`a`,`b`,`c`) USING BTREE
> > ) 
> > ```
> >
> > 复合索引树的数据结构：
> >
> > ![image-20191205153457874](/Users/wang/dev/environment/临时截图/QQ20191205-164932.png)
> >
> > (a,b,c)符合索引的B+树结构和其他索引完全相同，只有在叶子节点中存储的数据有所不同，可以发现`B+树是由列a组织起来的，叶子节点的数据是按照(a,b,c)三个索引的顺序进行排序的`。
> >
> > 介绍一个关键字explain，它用于分析SQL语句的执行，科普一下explain中的几个字段的含义：
> >
> > | 字段名 | 值    | 含义                                                     |
> > | ------ | ----- | -------------------------------------------------------- |
> > | type   | ALL   | 全表扫描                                                 |
> > |        | index | 扫描整颗索引树                                           |
> > |        | range | 扫描部分索引，索引的范围扫描                             |
> > |        | ref   | 使用非唯一索引或非唯一索引的前缀进行的查找               |
> > |        | const | 精确查找，单表中只有一个匹配行，例如主键和唯一索引的查询 |
> >
> > **情况一：**
> >
> > ```sql
> > 1. select * from t2 where a = 1;
> > 2. select * from t2 where a = 1 and b = 2;
> > 3. select * from t2 where a = 1 and b = 2 and c = 4;
> > ```
> >
> > 1. 按照条件a找到非聚集索引树中所有对应的主键回表查询。
> > 2. 按照条件a在索引树中找到对应记录，再在这些记录中筛选条件b，找到所有对应的主键回表查询。
> > 3. 同理
> >
> > **情况二：**
> >
> > ```sql
> > select * from t2 where a = 1 and c = 4;
> > ```
> >
> > 按照条件a在索引树中找到对应记录，接着将索引树上所有a = 1的记录遍历获得c = 4的记录对应的主键，回表。这条语句中索引命中了a，无法命中c
> >
> > 这里条件 c = 4没有使用到随机访问是因为，在确定a的值时只能保证b列是有序的（B+树的随机访问使用的是类似二分查找法的搜索算法，都是建立在有序的基础上的），c列无序，所以只能遍历。
> >
> > **情况三：**
> >
> > ```sql
> > 1. select * from t2 where b = 2;
> > 2. select * from t2 where b = 2 and c = 3;
> > ```
> >
> > 这两条SQL都会进行全表扫描，因为B+索引树只能由一个列来组织，复合索引中使用的是第一个列，因此条件只有b 和 c，那么SQL优化器会直接选择全表扫描记录。
> >
> > **情况四：**
> >
> > ```sql
> > 1. select * from t2 where a = 1 and b > 2;
> > 2. select * from t2 where a = 2 and b = 3 and c between 4 and 9;
> > 3. select * from t2 where a = 1 and b > 3 and c = 4;
> > 4. select * from t2 where a = 2 and c >2;
> > ```
> >
> > 1. 根据a = 1缩小索引树筛选范围后，再根据b > 2范围扫描
> > 2. 同理
> > 3. 根据a = 1缩小索引树筛选范围后，再根据b > 3范围扫描，再`遍历`范围扫描得出的全部结果筛选c=4，回表。`遇到范围扫描后就会退化为遍历扫描`。
> > 4. 根据a = 2缩小索引树筛选范围后，扫描范围内全部记录筛选c > 2的主键回表。还是因为确认a后只能保证b的顺序，c仍然是无序的 只能全部扫描。
> >
> > **总结：**
> >
> > - 在使用联合索引时必须根据联合索引的顺序来指定条件，否则索引可能失效
> > - 联合索引在遇到范围查询（>、<、between and、like右匹配和全模糊）后对后面的条件讲不再使用索引。
> > - 以上最根本的原因在于遇到范围查询或者跳过了复合索引中的某个列后，将无法保证下一个列的有序性

### 覆盖索引

> 覆盖索引不是一种索引，它是一种特殊的查询情况：
>
> ```sql
> CREATE TABLE `t2` (
> `id` int(11) NOT NULL AUTO_INCREMENT,
> `a` int(255) ,
> `b` int(255) ,
> `c` int(255) ,
> `d` int(255) ,
> PRIMARY KEY (`id`),
> KEY `index` (`a`,`b`,`c`) USING BTREE
> ) 
> ```
>
> ```sql
> -- 还是同样的表结构
> select c from t2 where a = 1 and b = 1;
> ```
>
> 在这条SQL中，查询的b、c列正好是复合索引中的列，那么在where条件正确的按照复合索引树检索时可以直接从复合索引树上拿到b和c的数据，那么就无需回表操作了，这也是提升性能的一种方式。
>
> ![](/Users/wang/dev/environment/临时截图/QQ20191209-155446.png)
>
> > 还是举个例子，在道具发放时，需要查询是否重复发放，需要从几个亿的大表里查询sn是否已经存在，那么肯定针对sn字段添加了索引，那么仅仅只是查重无需返回其他字段的数据，使用覆盖索引显然效率更高。

## 杂项

> **针对什么样的列建立索引？**
>
> > 1. 以区分度高的列建立索引，使用索引的目的在于快速定位缩小搜索的范围，如果使用了包含很多重复值的列，对检索效率的提升就不是很明显了
> > 2. 尽量不要使用长度过长的列作为主键（UUID，SN号等等），根据索引的数据结构可知，索引列占用的空间越大，随着数据量的增大，B+树深度也会随之增大，影响效率。
>
> **索引越多越好吗？**
>
> 每个索引就需要对应一颗B+树，添加索引有利于检索数据，但是在增删改数据时除了聚集索引树以外还需要对所有的辅助索引树进行维护，过多的索引会导致修改操作的效率变低。
>
> 可以在添加索引时考虑一下使用复合索引是否合适。
>
> > ```sql
> > CREATE TABLE `recharge` (
> >   `id` int(11) NOT NULL AUTO_INCREMENT,
> >   `account` int(255) DEFAULT NULL,
> >   `game_id` int(255) DEFAULT NULL,
> >   省略其他字段
> >   PRIMARY KEY (`id`),
> >   unique key 'index' ('account')
> > ) 
> > ```
> >
> > 例如在recharge表中需要查询某个账户的具体某个游戏下的充值记录，此时在account上已经有了一个索引，并且game_id在需求中通常只和account_id配合使用，此时就没有必要专门为game_id建立一个索引，可以将index修改为联合索引。

# 分区表

> **概念：**
>
> >  指的是按照一定的规则将表分解成多个更小更好管理的部分。
>
> **分区和分表的区别？**
>
> > 分区以后从逻辑上讲只有一个表，但是在物理上这个表可能由多个物理分区组成。分表则是将一张表拆分成了多个表，从物理或者逻辑上都已经是多个表了。

## 几种分区类型

> **Hash分区：**
>
> > 指定一个列或表达式为Hash分区的条件值，并指定分区数量。Hash分区的目的是将数据均匀的分布到预先定义的各个分区中。
> >
> > ```sql
> > create table t3(
> >     a int(11),
> >     b datatime,
> >     c int(11)
> > ) partition by hash(year(b)) 
> > 	partitions 4;
> > ```
> >
> > 分区的条件数据在Hash分区中`必须为整型(Integer)`，所以使用了year()函数取值
>
> **Key分区：**
>
> > Key分区和Hash分区类似，但是Key分区的条件中是支持除了text和BLOB（用来存储二进制大对象的字段类型）类型以外的所有数据类型分区，并且不允许用户自定义表达式进行分区。
> >
> > ```sql
> > create table t3(
> >     a int(11),
> >     b datatime,
> >     c int(11)
> > ) partition by hash(b)  --这里可以直接使用datatime类型了
> > 	partitions 4;
> > ```
>
> **Columns分区：**
>
> > Columns分区是在MySQL5.5之后开始支持的，它是对各种数据类型支持比较好。
> >
> > **range columns：**
> >
> > 按照指定的范围分区，例如 id 小于10的分在p0区，大于10小于20的分在p1区。根据时间分区比如time小于2010年的在p0区，大于2010小于2011的在p1区。
> >
> > ```sql
> > create table t3(
> >     a int(11),
> >     b datetime
> > )
> >     partition by range columns(b)(
> >     partition p2017 values less than('2017-01-01'),
> >     partition p2018 values less than('2018-01-01'),
> >     partition p2019 values less than('2019-01-01')
> > );
> > ```
> >
> > **list columns：** 
> >
> > list columns分区是按照离散的值，例如按照type列分类，type = (1,3,5,7)的记录在p1区，type = (2,4,6,8)的记录在p2区。
> >
> > ```sql
> > create table t3(
> >     a int(11),
> >     b int(11),
> > )
> >     partition by list columns(b)(
> >     partition p1 values in (1,3,5,7),
> >     partition p2 values in (2,4,6,8)
> > );
> > ```

## 正确使用

> **使用分区表有什么好处呢？**
>
> > - 在部分条件下可以提高SQL的执行效率（下文详细介绍）
> > - 分区表的数据更加方便维护，想要大批量的删除大量数据，可以使用清除分区的方式，也可以根据一个独立的分区进行优化、检查、修复等操作。（举个例子，对比清空的效率）
> > - 当单机MySQL出现了磁盘IO瓶颈，分区表可以将各个分区分布在不同的磁盘上，从而高效的利用多个磁盘。
> > - 对数据量特别大的单表，划分成分区表后可以独立的备份和恢复分区，这样可以大大缩短单次备份的时间。
>
> **什么情况下可以提高查询效率？**
>
> > 数据库查询的速度受到磁盘的制约，因此可以理解为越少的磁盘IO次数的查询也就越快。
> >
> > ```sql
> > create table t3(
> >     a int(11),
> >     b int(11),
> >     c datetime,
> >     key (a)
> > ) partition by range columns(c)(
> >   partition p2015 values less than('2015-01-01'),
> >   partition p2016 values less than('2016-01-01'),
> >   partition p2017 values less than('2017-01-01'),
> >   partition p2018 values less than('2018-01-01'),
> >   partition p2019 values less than('2019-01-01')
> > );
> > 
> > create table t4(
> >     a int(11),
> >     b int(11),
> >     c datatime,
> >     key (a)
> > )
> > 
> > -- t3和t4表结构完全相同，t4没有进行分区作为对照。表中都有500W条记录
> > -- 在未分区表中执行此SQL需要进行全表扫描
> > select * from t4 where c between '2018-01-01' and '2018-12-31';
> > 
> > -- 在以c列作为分区条件的分区表中，因为指定了c列为条件，查询优化器会到对应的分区查询，那么只需要从对应分区的100W数据中进行检索。此时效率是比不分区之前高
> > select * from t3 where c between '2018-01-01' and '2018-12-31';
> > 
> > -- 根据非分区条件的列为条件查询
> > select * from t4 where a = 666;
> > -- 在不分区表中查询过程为：从辅助索引树检索a=666的记录 ==> 获得主键回表查询对应数据。假设IO次数为2 + 3 = 5
> > -- 在分区表中过程为：
> > select * from t3 where a = 666;
> > -- 结果如下图，需要扫描p0~p9十个分区，即使假设每个分区查询开销为2次IO，5个分区下来就需要10次，效率是远不如单表设计的。
> > -- 原因在于表t4是按照c列分区，指定a列的值作为条件查询优化器无法判断它是在哪个分区，只好扫描全部分区。
> > ```
> >
> > 因此如果想要在分区表中获得不错的查询效率，最好在where条件中指定分区列作为条件以缩小查询范围，MySQL5.7以后支持指定分区查询。
>
> **分区表适合什么样的需求？**
>
> > **OLTP：** 联机事务处理，主要是基本的日常事务处理，进行即时的增删改，查询也只是查询少量的数据。
> >
> > **OLAP：** 联机分析处理，主要侧重于数据分析，为提供决策支持，例如报表，走势图等等，常常需要查询大量的数据。
> >
> > 分区表更加适合OLAP应用，因为OLAP应用经常需要频繁的扫描一张很大的表，例如有一张一亿行记录的表，其中有一个时间类型的列，需要从中获取近一年的数据，使用分区表可以很好的缩小全表扫描的范围。
> >
> > 如果是OLTP应用，通常都是通过索引获取几条记录即可，如果使用分区表设计不好会带来比较严重的性能问题。
> >
> > OLTP应用也并不是完全不能使用分区表，需要大家分析自身系统的需求来确定使用的方案，举个例子：
> >
> > > 在OLTP应用中，一张表中有几亿条陈年数据，但是老数据不可以删除，日常的业务中只会对近期新插入的记录进行update操作，此时可以根据时间按年分区，代码的SQL中只需要指定分区或者条件中加上时间戳，就可以在当年的分区中进行操作，并且针对往年数据进行备份和维护，可以避免繁杂的分库分表。

# 锁

> 锁和事务是区别数据库和文件系统的一个重要特性，锁机制用于保证对共享资源并发访问的正确性，本章介绍的是MySQL中InnoDB存储引擎下RR(Repeatable Read)隔离级别的各种锁机制。
>
> 了解了InnoDB中的锁机制有助于写出性能更好的SQL以及排查死锁、慢SQL等问题。

## 几种锁的类型

> - S Lock：共享锁
> - X Lock：排他锁
> - Gap Lock：间隙锁，记录之间间隙加的锁
> - Record Locks（RL）：记录锁，索引记录上的锁
> - Next-key Locks：临键锁，是记录锁和间隙锁的结合。
> - Insert Intention Locks：插入意向锁，数据行插入之前通过插入操作设置的间隙锁定类型。
> - Intention S：意向共享锁
> - Intention X：意向排他锁
> - Auto-inc Locks：自增锁，用于主键自增。



## S 锁/X 锁/R锁

> **共享锁（S 锁）:** 当表中某记录被加上S锁后，其他事务同样可以向该记录添加S锁，但是如果想要加X锁则必须等待所有S锁释放。
>
> **排他锁（X锁）:** 排他锁又被称为独占锁或互斥锁，每行记录或者每个表只能同时被一个事务加X锁，后续想要加X锁的事务会被暂时阻塞，直到超时或前面的事务释放锁后。
>
> **记录锁（R锁）：** 记录锁是行锁的另一种称呼，它是索引记录上的锁，记录锁可以分为共享记录锁和排他记录锁。
>
> **表锁：** 锁所有记录或者使用锁表语句的统称表锁。
>
> 行锁和表锁表示的是锁的细粒度，S锁和X锁表示的是锁的类型，可以有S记录锁，X记录锁，S表锁，X表锁等

> **快照读和当前读**
>
> **快照读：** 不加锁的读取方式，例如常见的查询语句
>
> > ```sql
> > 	select * from t1;
> > 	select c from t1 where a = 1 and b = 2;
> > ```
>
> **当前读：** 加锁的读取方式，例如加了关键字的特殊select操作或者update、insert、delete都属于当前读。这里加的锁又分为S锁和X锁
>
> > ```sql
> > select * from t1 lock in share mode;  -- 获取S锁
> > select * from t1 for update;					-- 获取X锁
> > insert into t1 values (...);					-- 后面再说
> > update t1 set (...) where (...);			-- 获取X锁
> > delete from t1 where (...);						-- 获取X锁
> > 
> > ```
> >
> > update和delete之前都需要进行类似select  ...... for update操作，寻找到要操作的记录并上X锁。insert比较特殊 ，后续再讲。

> **行锁：** 
>
> > **条件为主键时**
> >
> > ```sql
> > select * from T1 where id = 10 for update;
> > select * from T1 where id = 10 in share mode; -- 同理。这个加的是S锁
> > ```
> >
> > 此SQL中条件是主键的精确查询，所以只需要在该主键上加上X锁（X RL）。
> >
> > ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/HR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhp.png)
>
> > **条件为唯一索引时**
> >
> > ```sql
> > select * from T1 where id = 10 for update;
> > ```
> >
> > 当条件变为唯一索引时，select语句先去索引树中查找对应的记录为索引加上X锁，并根据主键name回表查询真正的记录行也要为主键加上X锁。
> >
> > ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE4NDc2OQ.png)
>
> “表锁”（并非真正意义的表锁，只是所有记录都上锁了）：
>
> > **条件为非索引时**
> >
> > ```sql
> > select * from T1 where id = 10 for update;
> > ```
> >
> > 此时因为条件并不走索引，所以会进行全表的扫描，所有的记录主键都会被加上X锁，此时的影响是最大的，其他需要加行锁的事务都需要等待表锁释放，严重影响吞吐量。这个是开发中要重点避免的情况，需要结合上文中的内容判断索引是否失效。
> >
> > ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/_16,color_FFFFFF,t_70.png)

## Gap锁/NK锁

> 间隙锁（Gap Lock），出现在RR隔离级别，用于避免**幻读**问题，它锁住的是记录中的间隔。
>
> >**幻读：** 在事务1第一次访问该范围记录后，另一个事务对此范围内数据进行了增删，第二次访问时发现多了或者少了记录，就像幻觉一样，所以称为幻读，侧重于记录的增删。
>
> 根据间隙锁的用途，我们可以猜测出，当where中的条件不唯一时才需要使用到间隙锁，也就是条件为非唯一索引和非索引时需要用到。

> ```sql
> update T1 set name = 'cc' where id = 10;
> select * from T1 where id = 10 for update;
> ```
>
> 如图所示id为普通索引，MySQL在id索引树上为id = 10加上了NK锁，索引加上了R锁，间隙之间加上了Gap锁，也就是为id加上了NK锁。主键B+树则是只为主键加上了R锁。
>
> ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/0001.png)
>
> 主键的索引树上不加Gap锁是因为主键是唯一的，不会造成幻读。

> ```sql
> update T1 set name = 'cc' where id = 10;
> select * from T1 where id = 10 for update;
> ```
>
> 此时的id已经不再是索引了，以id为查询条件会导致全表扫描，会对所有记录加上NK锁，对性能影响较大。
>
> ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/0002.png)

## Auto-inc Locks自增锁

> 它是一种特殊的表级锁，顾明思议，在并发插入时，InnoDB针对自增的主键提供线程安全的主键生成器，保证生成的主键没有重复并自增。

## 插入意向锁

> Insert Intention Locks是间隙锁的一种，它是专门针对insert操作产生，目的是提高并发插入。
>
> 为了方便理解将insert和update加入的锁进行一个对比：
>
> 表数据和结构如下：
>
> | id   (primary key) | name | dpt_num   (key) |
> | ------------------ | ---- | --------------- |
> | 1                  | a    | 1001            |
> | 2                  | b    | 1004            |
> | 3                  | c    | 1007            |
>
> 此时事务T1执行：
>
> ```sql
> start transaction;
> update t1 set name = 'bbb' where dpt_num = 1004;
> -- 还未提交
> ```
>
> 将会对dpt_num = 1004的所有记录加上NK锁，Gap锁定的范围为(1001,1004)和(1004,1007)。此时在这个区间内是不允许其他事务进行增删改的。
>
> 若插入操作也是如此，插入一条就需要锁定上下范围内的间隙，那么这个范围内的插入就变成了串行插入，效率是十分低下的，插入意向锁就是为了解决这个问题。
>
> ```sql
> start transaction;
> insert into demo (name,dpt_num) values ('d',1003);
> // 尚未提交
> ```
>
> - 普通的Gap Lock 不允许 在锁定范围内插入数据
>
> - 插入意向锁Gap Lock 允许在锁定范围内插入数据
>
>    例如事务执行如上的插入语句，其他并发插入事务是允许在(1001,1003)区间并发插入的。

## IS和IX

> 意向锁是表级别的锁
>
> **IS（意向共享锁）：** 事务想要获得一张表中的某几行共享锁
>
> **IX（意向排他锁）：** 事务想要获得一张表中的某几行排他锁
>
> 表锁和行锁都有各自的S锁和X锁，并且性质相同，可以理解为表锁和行锁是类别，各自的X锁和S锁是具体的实现。
>
> **意向锁存在的意义是让InnoDB支持表锁和行锁共存，并提交加锁的效率。**

> 试想一下，如果有事务为表加了X锁，那么此时必然不允许加行锁进行修改。反过来，如果表中某一行被加了行锁，那么就不能为整个表上表锁，并且如果每次加表锁都需要遍历全表查看是否有行锁存在是非常低效的。
>
> 在InnoDB中，想要对下层对象上锁，则必须先为上层对象上锁。我们模拟一下加锁的几种情况：
>
> 假设T1想要对表1中某个记录加X锁，那么事务T1必须先在表1中加上一个IX锁才能继续去为记录加锁。
>
> 假设T2想要对表1加X锁，那么需要查看表中是否有不允许共存的意向锁存在。
>
> | 兼容性 | IS   | IX   | S    | X    |
> | :----- | :--- | :--- | :--- | :--- |
> | IS     | 兼容 | 兼容 | 兼容 | 互斥 |
> | IX     | 兼容 | 兼容 | 互斥 | 互斥 |
> | S      | 兼容 | 互斥 | 兼容 | 互斥 |
> | X      | 互斥 | 互斥 | 互斥 | 互斥 |
>
> 表中的S和X指的是表锁。意向锁不会阻塞行锁。
>
> 例如：IX和IX兼容是因为表中允许多个行锁同时存在，同理IS和IX也兼容。而IX和X互斥是因为记录行被锁时是不允许锁表的。IS和S兼容是因为共享锁不涉及修改所以允许共存。

## 锁与事务

### 如何防止幻读？

> **幻读：**指的是在同一事务内重复访问同一个范围的记录时，出现多了或者少了记录，就像幻觉一样，重点强调增删。
>
> 那么简单来说避免在一个事务update的范围内出现增删就行了，Gap锁防止插入， R锁锁定索引防止删除。

### 如何防止不可重复读

> **不可重复读：** 指一个事务内多次访问同一数据，在第一次访问后，另一个事务访问了此数据并进行了修改，那么第二次访问时前后的结果不一致，（重点在于修改）称为不可重复读。
>
> 这个就比较简单了，update时添加了X锁在索引上，另一个事务想要同时更改就无法添加对应的锁了，从而实现了串行修改同一条记录。

**如何避免脏读？**

> **脏读：** 一个事务在修改数据后还没有提交，此时另一个事务访问了这个数据，它读取到了这个修改还未提交的脏数据，称为脏读。
>
> 脏读的避免和锁就无关了，使用的是MVCC机制。
>
> ## MVCC
>
> > **概述：**
> >
> > > (Multi-Version Concurrency Control)多版本并发控制，它是一种并发控制方法，在大多数情况下避免了加锁操作，从而提高效率。`MySQL中MVCC只能在Repeatable Read（读可重复读）、Read Committed（读可提交）这两个隔离级别下工作`
> >
> > **用途：**
> >
> > > MVCC机制使快照读无需加锁，通俗点说就是普通的select语句是无需参与锁竞争的，可以和其他任何操作并发执行。其次就是可以避免脏读和不可重复读的问题。
> >
> > **原理：**
> >
> > > 为了实现MVCC，InnoDB在表中有两个`隐藏列`，这两个列分别保存了这行记录的`当前版本号`和`删除版本号`。每当开始一个事务时，事务开始时刻的`系统版本号`就会作为`事务版本号`，因此每个事务的版本号都是不同的。
> > >
> > > 下面举几个例子来理解（隔离级别RR）：
> > >
> > > **Insert：**
> > >
> > > > InnoDB将新插入的行的行版本号设定为该事务的版本号
> > >
> > > **Delete:**
> > >
> > > > InnoDB为删除的记录行设置删除版本号为该事务的版本号
> > >
> > > **Update:**
> > >
> > > > InnoDB会先插入一条新记录，并将新纪录的版本号和旧记录的删除版本号设为该事务的版本号。
> > >
> > > **Select：** 
> > >
> > > > 对于Select语句InnoDB会按照以下两个条件检查记录：
> > > >
> > > > - 只查找版本号 `小于等于` 与当前事务的版本号的记录，这样可以确保事务读取的行是在此事务之前修改过并提交的，或者是该事务自身插入或修改过的行。
> > > > - 行的删除版本号要么未定义，要么大于当前事务的版本号，确保该事务读取到的行在事务开始之前还没有被删除
> >
> > **优缺点：**
> >
> > > **优点：** MVCC机制使得快照读的查询成为无锁操作，大大提高了数据库系统的并发读写能力。
> > >
> > > **缺点：** 为了实现多版本，innodb必须对每行增加相应的字段来存储版本信息，同时需要维护每一行的版本信息，而且在检索行的时候，需要进行版本的比较,因而降低了查询的效率；innodb还必须定期清理不再需要的行版本，及时回收空间，这也增加了一些开销。但是总体利大于弊。

## 几种不好的习惯

> **在循环中提交**
>
> 一般在程序代码中不容易出现循环提交，但是在存储过程中是比较容易被忽略的
>
> ```sql
> CREATE PROCEDURE demo(IN count INT)
> BEGIN
> DECLARE s INT DEFAULT 1;
> 
> WHILE s <= count DO
> INSERT INTO t1 (str) VALUES(s);
> SET s = s + 1;
> --COMMIt;
> END WHILE;
> 
> END;
> ```
>
> 这段存储过程看似没问题，实际上忽略了InnoDB是默认自动提交事务的，所以在WHILE循环中是否添加COMMIT结果都是循环提交。
>
> 缺点也是显而易见的，假设循环100次，在第51次时出现错误回滚了，那么之前的50条记录已经存入数据库中又应该怎么处理呢。其次是性能问题，测试如下：
>
> 执行循环1万次，分别测试循环提交和批量提交所花费的时间。
>
> ```sql
> CREATE PROCEDURE batch(IN count INT)
> BEGIN
> DECLARE s INT DEFAULT 1;
> START TRANSACTION;
> 
> WHILE s <= count DO
> INSERT INTO t1 (str) VALUES(s);
> SET s = s + 1;
> END WHILE;
> 
> COMMIT;
> END;
> ```
>
> 循环提交：
>
> > ![image-20191203152631960](/Users/wang/dev/environment/临时截图/image-20191203152631960.png)
>
> 
>
> 批量提交：
>
> > ![image-20191203152720138](/Users/wang/dev/environment/临时截图/20191203152720138.png)
>
> 

## 死锁

> **死锁：**两个事务相互等待对方资源的释放而阻塞等待，若无外力作用，他们都无法继续执行。
>
> 死锁的情况有很多种，常见比较容易排查的有SQL语句执行顺序导致的死锁，比较隐秘难以排查的有高并发下SQL语句锁的竞争导致的死锁等。
>
> MySQL种对死锁的处理很简单，检测到死锁后InnoDB引擎会选择undo量最少的事务进行回滚（undo日志记录的是回滚数据，undo量少的回滚成本小），强制一个事务释放资源。

> 例1：
>
> ![在这里插入图片描述](/Users/wang/dev/environment/临时截图/0003.png)
>
> 这个例子中两个事务都是只有一条SQL，但是依然可能导致死锁，原因就在于InnoDB引擎对索引是逐个加锁的，并不是一个原子操作。
>
> session1根据索引name作为条件，那么会先在name索引树上对索引进行加锁，接着获取到主键后回到主键索引树进行逐个加锁，session2同理，只是将where的条件换成了另一个索引。死锁出现的原因就在于他们在回表操作时，逐个加锁，就存在顺序的问题，S1先为id = 1 加锁，S2为6加锁，接着S1为6加锁被阻塞，S2再为1加锁也被阻塞，进入了死锁。

> 例2：
>
> **背景：** 在对公司程序Dao层重构后进行数次压力测试后发现，日志中有几率出现大量死锁异常，并且特定在每次压测后清理数据库的情况下才会出现，在运行一段时间后便不会再出现死锁异常。
>
> 事务隔离级别为RR
>
> 事务T1简称插入事务：
>
> | SQL                                          | 备注                        |
> | -------------------------------------------- | --------------------------- |
> | INSERT INTO INFO (xxxx) VALUES(xxxxx)        | 向INFO表中插入一条记录      |
> | INSERT INTO INFO_DETAIL (xxxx) VALUES(xxxxx) | 向INFO_DETAIL中插入一条记录 |
>
> 事务T2简称更新事务：
>
> | SQL                                                          | 备注                     |
> | ------------------------------------------------------------ | ------------------------ |
> | UPDATE INFO_DETAIL SET STATUS = '2' WHERE INFODETAIL_ID IN ('1','2',省略) | 向INFO表中插入一条记录   |
> | UPDATE INFO SET STATUS = '2' WHERE INFO_ID IN ('1','2',省略) | 根据主键批量更新STATUS值 |
>
> 根据日志中MyBatis的错误信息锁定到出现死锁的一个事务，接着在MySQL控制台中输入命令：
>
> ```sql
> -- 此命令可以获取当前状态下锁请求的信息，和最后一次死锁发生的日志
> show engine innodb status\G;
> ```
>
> ```sql
> ------------------------
> LATEST DETECTED DEADLOCK
> ------------------------
> 2019-11-27 11:41:23 7f5c5de2d700
> *** (1) TRANSACTION:
> TRANSACTION 1075171, ACTIVE 1 sec fetching rows
> mysql tables in use 1, locked 1   
> LOCK WAIT 12 lock struct(s), heap size 13864, 109 row lock(s), undo log entries 98
> MySQL thread id 180, OS thread handle 0x7f5c5db99700, query id 3689466 192.168.16.1 root updating
> UPDATE
>      INFO
>      SET STATUS = '2',
>      UPDATE_TIME = (SELECT NOW())
>      WHERE
>      INFO_ID IN
>       ('1','2',省略 )
>      
> *** (1) WAITING FOR THIS LOCK TO BE GRANTED:
> RECORD LOCKS space id 6 page no 3 n bits 128 index `PRIMARY` of table `db`.`INFO` trx id 1075171 lock_mode X waiting --等待INFO表上某主键的一个X RECORD LOCKS
> Record lock, heap no 55 PHYSICAL RECORD: n_fields 20; compact format; info bits 0
> 0: len 4; hex 8008891d; asc     ;;
> 
> 
> *** (2) TRANSACTION:
> TRANSACTION 1075211, ACTIVE 0 sec inserting
> mysql tables in use 1, locked 1
> 4 lock struct(s), heap size 1184, 2 row lock(s), undo log entries 1
> MySQL thread id 162, OS thread handle 0x7f5c5de2d700, query id 3689841 192.168.16.1 root 
> INSERT INTO INFO_DETAIL 
>      (INFO_ID,省略)
>      VALUES
>          ('2',省略)
> *** (2) HOLDS THE LOCK(S):
> RECORD LOCKS space id 6 page no 3 n bits 128 index `PRIMARY` of table `queue`.`INFO` trx id 1075211 lock_mode X locks rec but not gap -- 持有INFO表上某主键的X RECORD LOCKS无间隙锁
> Record lock, heap no 55 PHYSICAL RECORD: n_fields 20; compact format; info bits 0
> 
> *** (2) WAITING FOR THIS LOCK TO BE GRANTED:
> RECORD LOCKS space id 7 page no 3 n bits 120 index `PRIMARY` of table `queue`.`INFO_DETAIL` trx id 1075211 lock_mode X insert intention waiting
> Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0
> -- 等待表INFO_DETAIL上的一个插入意向锁（lock_mode X insert intention waiting）
> 
> -- 处理结果是回滚了事务2
> *** WE ROLL BACK TRANSACTION (2)
> ```
>
> 这里出现了一个比较奇怪的现象:
>
> > - 在RR隔离级别下是不会出现脏读的，那么Insert事务在提交之前新插入的记录是不会被select到的，那么为何Update事务会需要获取一个未提交的记录的锁呢？
> > - 并且以主键作为where条件精确搜索，由于主键的唯一性InnoDB只会为其加上Record Lock，那么为何Insert事务会需要去获取插入意向锁(特殊的Gap锁)呢？
>
> 结合死锁日志的分析，和死锁出现的背景来分析，仅仅在表清空时出现，说明数据量小的时候才有可能出现，结合上奇怪的现象，那么只有一种可能就是update语句锁表了，那么焦点就集中在关键字in上了，查阅资料发现，`in中的条件数量如果大于表记录数的一半就会自动变为遍历全表`，线上设定每次批量提交100条，那么也就印证了在空表时压测才有可能出现。
>
> 综上整理一下思路：
>
> ![有向图](/Users/wang/dev/environment/临时截图/20191202164008310.png)
>
> 在表中数据少于200时，Update事务走全表扫描，会对全表的索引记录加上NK锁，这个是发生死锁的核心原因。结合图中的执行顺序也就不难理解了。

> **死锁发生概率：** 每个事务操作的数量越多发生死锁的概率越大，表内数据越小发生的概率也越大。不难理解事务中操作多了以后事务的并发执行会被各种穿插，表内数据少则很容易操作到相同的记录。
>
> **死锁的处理：** 最简单的方法就是超时，两个事物进入死锁后等待到设定的超时时间就回滚其中一个事务，这么做的好处就是简单，缺点也是显而易见会导致资源的浪费和连接数的占用。
>
> **wait-for graph：** 和超时等待相比，等待图是更为主动的死锁检测机制，InnoDB也是采用这种方式检测死锁。它利用了数据结构“图”的特性描述事务之间的等待关系。在每个事务请求锁发生等待时，就会判断图中是否存在回路，如果存在则表示发生了死锁，InnoDB就会选择undo量最小的事务回滚。
>
> ![q](/Users/wang/dev/environment/临时截图/image-20191202141621255.png)
